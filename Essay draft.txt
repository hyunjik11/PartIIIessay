0.ABSTRACT

Collaborative filtering defines a branch of techniques for tackling the following supervised learning problem: 
making predictions(filtering) about the preferences of a user, based on information regarding the preference of many users (collaborating).
Having obtained such predictions, it is then a simple task to build a recommender system for users; we simply recommend the items with highest preference by a given user.
In this paper, we first introduce the three most successful Bayesian algorithms for collaborative filtering on the Netflix dataset. These are Probabilistic Matrix Factorization (PMF), its variant Bayesian PMF(BPMF), and Variational Bayes (VB). We describe their mechanisms in full, fleshing out the missing details in the original papers. We then give results of authentic numerical experiments for direct comparison of their empirical performances, as well as speed. Moreover we investigate their performances on a small subset of the data for users with few movie ratings, where prediction is known to be most difficult. Finally, we show results of experiments using combinations of these algorithms for improved performance on the Netflix data.

Comments: Would be nice to include some results in the abstract.

1.INTRODUCTION

Prediction of user ratings applies to various contexts, such as for online videos, books and other commercial products. In this paper, we will focus on movie ratings by users provided by the Netflix dataset, the largest publicly available movie rating data. This was publicised during the Netflix Prize in 2009, a competition organised by Netflix for the most accurate recommender system that predicts user ratings, given the user rating history [2 in VB]. Netflix provided just over 100,000,000 ratings from around 480,000 randomly chosen users and around 18,000 movies which they have rated, each with an integer between 1 and 5, collected between 1998 and 2005. The task is to predict a prespecified set of 3 million unobserved ratings (test set) from these users over these movies. The metric for error is the root mean-squared error (RMSE) over the test set. We use Cinematch, Neflix's own system, with RMSE 0.9474 as a baseline.

Let us introduce some notation and terminology to begin with. Let there be N users with M movies. Each user u_i rates movie m_j with rating R_ij /in {1,...,K}. Hence the rating matrix R is incomplete. In fact, it is incredibly sparse as most users will only have rated a small fraction of the total set of movies. Collaborative filtering aims to fill R by only using the already filled entries (observed ratings) of R. The underlying assumption for this matrix completion that if user A has a similar rating as user B on a movie, then A is more likely to share B¡¯s rating on a different movie than another user selected at random. 

This approach is contrary to content-based filtering, which predicts user ratings based on attributes of movies such as genre,number of viewings,director,cast etc. There are several drawbacks of content-based filtering, such as limited content analysis and over-specialisation, but the main difficulty in applying content-based filtering to a dataset as large as the Netflix data is that it requires building user profiles; this highly memory intensive task is unsuitable for large data sets. Also it is not so clear as to how detailed the profiles should be made and how the weights should be given to different attributes of films. This is why the vast majority of approaches to the Netflix problem are based on collaborative filtering, which deals with a problem of reduced complexity, and thus can be seen as a step towards automation.

Collaborative filtering can further be classified into two approaches: memory-based and model-based. The former uses the rating data  to calculate similarities between users or between movies, and makes predictions based on these similar users/movies, called neighbours. Then a certain function of the similarity and observed ratings is used to predict ratings. A popular example would be k-NN, where a weighted average of the k nearest neighbours' ratings on a movie is the prediction. The similarity measure used is normally not a metric as the triangle inequality does not hold (two users with preferences dissimilar to a third user may have similar preferences). This implies that we need a similarity measure between all pairs of users, the number of which is O(N^2), hence giving rise to scalability issues for big data such as the Netflix dataset. Moreover the sparsity of the data implies that there could only be a handful of truely similar users, severely limiting the set of movies for which a sensible prediction can be made. A more fundamental problem is that there is no explicit statistical model, so very little insight in the data is gained when creating a memory-based prediction system. 

On the other hand, model-based collaborative filtering methods use the rating data to train a statistical model, which can then be used to make predictions. There have been various models in the literature even before the Netflix Prize, notably cluster based models ([5,6,7,8] of CL via Gaussian PLSA paper) and probabilistic models such as [Gaussian pLSA]. These algorithms seem to show impressive results on standard datasets such as MovieLens and EachMovie. However we must bear in mind that these datasets have had all users with fewer ratings than a lower threshold removed, which are intuitively the most difficult cases. The Netflix data, on the other hand, is very imbalanced: there coexist infrequent users with less than 5 ratings and frequent users with more than 10,000 ratings. Moreover the previously mentioned data sets are orders of magnitudes smaller than the Netflix data in size, and it has become apparent that scalability is also one of the main issues with these existing models.

The breakthrough for scalable model-based approaches was the clever idea of applying matrix factorisation to the rating matrix. We factorise the N by M rating matrix R as a product of two matrices U and V^T, where U and V are N by D and M by D respectively, where D is a positive integer to be chosen. They are referred to as the user matrix and the movie matrix. Instead of learning R directly, we build a model for R in terms of U and V, and try to learn U and V so that the error on the observed ratings is minimised. Then we simply fill in the blanks of R by multiplying out U and V^T. The intuition behind this model is that there are only a few factors which affect the user's rating of a movie. So each row of V can be interpreted as the D features of a movie which affect the rating of users, and then each row of U would correspond to the user's weights given to each feature. Note here that the choice of a suitable dimension D is important. If D is large, we will always be able to choose U and V such that we have perfect predictions on observed ratings, the training data. This hence leads to overfitting, as well as being computationally infeasible. So we want D to be small, but not too small as then it might not capture all the different features responsible for a rating.

From a mathematical point of view, we note that if R=UV', then rank(R)<=D. Hence finding U and V is equivalent to finding the best rank D approximation to R, in the squared error sense. It is a standard result in linear algebra that this can easily be obtained from the Singular Value Decomposition (SVD) of R, commonly referred to as the Matrix Approximation Lemma [The approximation of one matrix by another of lower rank].

Here: Equations for SVD in latex.

However, the problem with our rating matrix is that it is incomplete - some entries are missing. The above result holds for R complete. This seemingly small modification results in a non-convex optimisation problem, and an approach different to standard SVD computation needs to be employed. Nonetheless there do exist SVD extensions which deal successfully with this problem, such as the use of the EM algorithm where the unobserved entries are modelled as latent variables [10 of VB], Gradient Boosting with successive rank 1 updates to U and V, and many other variants. These approaches have shown to be successful on the Netflix data. However, these procedures have also shown to overfit easily due to the extreme sparsity of the rating matrix, and require careful tuning of parameters such as the number of dimensions D for proper regularisation.

This paper describes Bayesian models which are less prone to problems of overfitting, as well as being superior in performance to the aforementioned SVD algorithms. In section 2 we introduce the Probabilistic Matrix Factorization(PMF) algorithm, which uses MAP estimation on the simplest Bayesian model with a independent Gaussian distributions on each entry of R along with independent Gaussian priors on the rows of U and V. In section 3 we give an extension of PMF called Bayesian PMF, which uses Monte Carlo estimation on a hierarchial Bayesian model. In section 4, we describe the Variational Bayesian approach (VB) which replaces MAP estimates of U,V in PMF with their expectations, and uses variational inference to approximate these values. In section 5, we compare their empirical performances: their RMSE on the test set, the RMSE for ratings of infrequent users with less than 10 ratings, and the RMSE for different combinations of these algorithms. We also cover with caution the running times for these algorithms. Wherever possible we evaluate the results and offer possible explanations to provide insight for each algorithm. Finally in section 6, we discuss other models which deserve interest, such as those which use Restricted Boltzmann Machines and Gaussian Processes. We also provide further scope of research in the domain of large-scale collaborative filtering.

2.PROBABILISTIC MATRIX FACTORIZATION (PMF)

-include figure 1 in paper - graphical model

We use the notation in the Introduction, along with U_i for the ith row of U and V_j for the jth row of V. Since we wish U_i V_j' to estimate R_ij, the most natural Bayesian model would begin with the following conditional distribution on each R_ij:
R_ij|U_i,V_j ~ N(U_i V_j',sigma^2)
Also for simplicity in calculation, we assume that R_ij, conditioned on the values of U and V, are independent random variables. Hence:

Equation (1) , but use (ij) for the bounds of the product instead of using the indicators I_ij.

where (ij) ranges over all user/movie indices in the training set.
The paper does not mention that although R_ij is in fact a discrete, integer-valued RV, it is modelled by a continuous distribution. This model is, however, justified as there is no requirement that the prediction must also be integer-valued. It is in fact beneficial to model R_ij as a continuous RV, since then non-integer part of the prediction would be helpful in distinguishing the top recommendations from the good ones.

Moreover we set U and V to be random variables, on which we put Gaussian priors with zero mean and spherical covariance(a multiple of the identity matrix):

Equation (2)

These priors can be interpreted as a natural regularisation induced by our Bayesian model, which can help reduce overfitting. We will see below how the values of sigma can be interpreted as the degree of regularisation. Here one could also raise the issue whether it is sensible to use a spherical covariance as opposed to, say, a diagonal covariance or even a full covariance matrix. The paper confirms that emprically, this generalisation does not give much of an improvement in performance, hence the paper adheres to a spherical variance for the sake of simplicity. 

By Bayes' Theorem the log posterior of U and V can be calculated as:

Equation (3)

where C is some constant independent of the parameters. Maximising the log-posterior over U and V is then equivalent to minimising the sum of squared errors with quadratic regularisation terms:

Equation (4)

where lambdaU= , .... . . Although unmentioned in the paper, this form of quadratic regularisation, as in ridge regression, is intuitively the type of regularisation we would want to avoid overfitting; with careful optimisation, U and V with large entries may have low error on observed entries of R. Then, however, predictions for unobserved entries can become extreme, and due to the extreme sparsity of R, this will become a major issue when trying to predict many ratings, as is the case for the Netflix Prize. So it is sensible to have U and V small by penalising their L2 norms. This demonstrates that the use of a Gaussian prior on U and V, and using a maximum a posteriori (MAP) estimate is advantageous for three reasons: First, it automatically provides sensible and intuitive regularisation. Secondly, this approach to regularisation is much more flexible than simply penalising the L2 norms of U and V, since we may use priors with diagonal or even a full covariance matrix with adjustable means. And thirdly, the parameters whose values we need to fix is reduced from three (sigmas) to two (lambdas), making easier the tuning parameter selection and hence effective regularisation.

Now the objective in (4) is minimised wrt U and V by gradient descent, hence U,V converge to a stationary point. 

Equation on top of page 6, but use epsilon instead of lambda

where the epsilon is the learning rate and p is the momentum term, which is used to avoid slow convergence due to heavy zig-zagging behaviour. Since del(E) is continuous, we may guarantee convergence to a local minimum by adjusting epsilon at each step of the iteration such that the Wolfe conditions are satisfied. However this is computationally demanding due to the large number of variables (N rows in U, M rows in V), so we use a constant learning rate. 

Instead of updating U and V after each epoch (sweep through the entire training set), we divide the data into mini-batches of size 100,000 and update U,V after each batch in order to speed up training. Also since we are updating by going through the training set in order, it is important that we randomly permute the data after each epoch.

Through extensive experimentation, the paper claims that the use of values epsilon=0.005,momentum=0.9,lambda_U=0.01, lambda_V=0.001 gives rise to best results. So we use these values for the numerical experiments.

-Algorithm pseudocode-

inputs: training_data,test_data,maxepoch,D (optional: U,V,U_inc,V_inc)

Initialise epsilon,lambda_U,lambda_V,momentum,numbatches

If U,V,U_inc,V_inc missing
	initialise U,V st each entry is 0.1*N(0,1) and U_inc, V_inc st each entry is 0
end

for epoch=1:maxepoch
	permute training_data
	for batch=1:numbatches
		pred_out=predicted ratings for the user/movie pairs in batch, using current U and V
		Set dE/dU_i and dE/dV_j to 0 for all i and j
		Update dE/dU_i and dE/dV_j by adding on terms contributed by the pairs/ratings in batch
	end
	U_inc=momentum*U_inc-epsilon*dE/dU_i
	V_inc=momentum*V_inc-epsilon*dE/dV_j
	Increment U and V by U_inc and V_inc respectively
end

----------------------

The paper suggests various modifications to the model. First it raises the point that with zero mean Gaussian priors on U and V, the expected value of U_iV_j' is zero a priori, whereas we would like a number between 1 and K. Hence the paper suggests first mapping the ratings linearly to [0,1] by t(x)=(x-1)/(K-1) and modelling the mean of R_ij|U_i,V_j as sigmoid(U_iV_j'), so that the expected value is 1/2 apriori. Although the paper claims this modification renders an improved RMSE, we show later in section 5 through various experiments that not only is this computationally more expensive but also does not bring about better results, followed by a possible explanation of why this is. Furthermore, we discuss in section 6 an approach which tries to learn the function that should be applied to U_iV_j', using the non-parametric approach of Gaussian Processes.

Note that regularisation is ocurring in two different ways in our algorithm. Firstly, our choice of D is crucial in controlling overfitting, since as mentioned in the Introduction, given sufficiently many factors the model can have arbitrarily small error on the training data. However the trouble with our Netflix data is that it is very imbalanced; the number of observations are significantly different over the rows and columns. Hence any single D will be too high for some rows but too low for others, so there is a limit as to how effective this form of regularisation can be. So our second form of regularisation also plays an important role, which is through choosing the parameters lambda_u and lambda_v. The simplest way to find suitable parameters would be to train each model on a set of candidate parameters, and choose the pair of values which gives the lowest RMSE. However this is computationally expensive as numerous models have to be trained. So the paper suggests a procedure called Adaptive Priors PMF, whereby priors are placed on sigma,sigma_U and sigma_V, creating a hierarchial Bayesian model and so regularisation is now controlled by hyperparameters. Then an MAP estimate of U,V,sigma,sigma_U,sigma_V are obtained again gradient descent on the follwing posterior:

Equation (6)

We will further discuss this model in the next section on Bayes PMF. 

-brief statement of constrained PMF model?


3.BAYESIAN PROBABILISTIC MATRIX FACTORIZATION USING MARKOV CHAIN MONTE CARLO


For the Bayesian PMF model, we build on the same conditional distribution for R as in the PMF model:

(same equation as in pmf model, except with alpha^-1 instead of sigma)

One problem of the PMF model described in the previous section is the need for manual setting of the regularisation parameters \lambda_U and \lambda_V, which is essential in controlling overfitting. So as suggested in the Adaptive Priors PMF model, it would be sensible to create a further level of parametrisation, by placing prior distributions over the paramters of U and V, so that they are selected automatically by hyperparameters. In the Bayes PMF model, however, the priors on U and V are now non-centered Gaussians, with further priors on both the mean and the precision, the inverse of the covariance matrix:

Equations (5,6,7,8)

-include figure 2 in paper - graphical model

Note that we have placed Gaussian-Wishart(G-W) priors on \Theta_U={\mu_U,\Lambda_U} and \Theta_V={\mu_V,\Lambda_V}. This is because the G-W distribution is the conjugate prior to the Gaussian likelihood with unknown mean and unknown precision. In other words, this choice of prior ensures that the posterior of \Theta_U \Theta_V are also G-W, making it easy to sample from these conditional distriubtions as we shall see later on.
We define  \Theta_0={\mu_0,\nu_0,W_0,\beta_0), and use the values mu_0=0,nu_0=D,W_0=I,\beta_0=2,\alpha=2 as in the paper.

Another potential problem of the PMF algorithm is that we use MAP estimates for U and V, which is prone to overfitting as it finds a single point estimate of U and V; it does not take into account all the information in the posterior distribution of U  and V. Instead, it would be beneficial to put a distribution on the parameters \Theta_U,\Theta_V, and integrate out all random variables but R from the joint distribution to obtain its expected value.This is precisely what the paper attempts to do; the predictive distribution of a new rating R_ij* is obtained by integrating out U,V and the model paramters from the joint:

Equation(9)

Exact evaluation of this distribution is analytically intractable, hence we must rely on approximate inference using a Monte Carlo approximation:

Equation(10) except change K to T and k to t

where T is the number of epochs for our algorithm. Here each sample {U_i^(t),V_j^(t)} is generated by running MCMC on a Markov chain with stationary distribution equal to the posterior of {U,V,\Theta_U,\Theta_V}. We use Gibbs sampling with a prespecified number of jumps for each sample of {U_i^(t),V_j^(t)}. MCMC methods are rarely used for large-scale data analysis as they are considered too computationally demanding. Nonetheless in this model the use of G-W priors, which gives closed form posteriors and thus allows fast sampling from the posterior, gives rise to a computationally feasible implementation of Gibbs Sampling.

The conditional distribution of U given all other random variables and hyperparameters is:

Equations(11,12,13) 

with a similar conditional distribution for V. Note that the conditional distribution over U factorises into a product of conditional distributions for each row U_i, and hence the sampler can easily be speeded up by sampling in parallel. This will indeed be a significant improvement since we must invert a D by D matrix for each sample of U_i, and O(D^3) operation. There is a large number of users, hence this is the main bottleneck in reducing running-time. So we see how the independence assumptions for each entry of R and each row of U and V are all the more beneficial for computational reasons.

The conditional distribution of the parameters \Theta_U is given by the G-W distribution:

Equation (14)

and similarly for \Theta_V.

-Algorithm pseudocode-
Inputs: training_data,test_data,R,maxepoch,D,n_samples,U,V (use U,V obtained from PMF)

Initialise \alpha,\mu_U,\mu_V,\Lambda_U,\Lambda_V,W_0,\beta_0,\nu_0,\mu_0
Initialise prediction = prediction of observed values for initial U,V
for t=1:maxepoch
	sample \Theta_U^t={\mu_U^t,\Lambda_U^t} from the G-W posterior distribution \Theta_U|U,\Theta_0
	sample \Theta_V^t={\mu_V^t,\Lambda_V^t} from the G-W posterior distribution \Theta_V|V,\Theta_0
	for gibbs=1:n_samples
		for i=1:N (can be parallelised)
			sample \U_i^(t+1) from the the posterior U_i|R,V^t,\Theta_V^t
		end
		for j=1:M (can be parallelised)
			sample \V_j^(t+1) from the the posterior V_i|R,U^(t+1),\Theta_U^t
		end
		prediction=prediction+prediction from update U and V
	end
	prediction=prediction/(maxepoch+1)
end
----------------------

Note that we provide the output of PMF as the initial values for U and V in our algorithm. There is scope for experimentation by initialising U and V differently.


4. VARIATIONAL BAYESIAN APPROACH TO MOVIE RATING PREDICTION

The model for this new approach is precisely the model in PMF with diagonal covariance priors on $U$ and $V$:

Equation (1) in PMF , but use (ij) for the bounds of the product instead of using the indicators I_ij, and also use tau instead of sigma
Equation (2) in PMF , but use Sigma=diag(sigma_1,...,sigma_D), Rho=diag(rho_1,...,rho_D)

The novelty of this approach is that we use a different method for inference. We mentioned in the last section that a problem of the PMF algorithm is that it uses and MAP estimate $argmax_{U,V} p(U,V|R)$ for $U$ and $V$. These are point estimates, which are prone to overfitting. So in the Bayes PMF algorithm, we used MCMC to estimate the expected value of $R$ by integrating out all other random variables from the joint. For the VB approach, we attempt to estimate instead the values $\hat{U}=\mathbb{E}(U|R)$ and $\hat{V}=\mathbb{E}(V|R)$, and use $\hat{U_i} \hat{V_j}^T$ as the prediction for $R_{ij}$. Now in order to evaluate this expectations, we need to be able to compute:

p(U,V|R)=p(R|U,V)p(U)p(V)/p(R)=p(R|U,V)p(U)p(V)/integraloverU,V[p(R|U,V)p(U)p(V)]

However the denominator is both analytically and computationally intractable due to the high dimensionality of $U$ and $V$, hence exact inference is not possible. Hence we resort to variational inference where we lower bound the marginal log-likelihood and try to maximise this bound, while at the same time using and approximation $Q(U,V)$ of $p(U,V|R)$ to estimate $\mathbb{E}(U|R)$ and $\mathbb{E}(V|R)$. The variational free energy [1 in VB] is:

Equation (13,14,15) with R instead of M

where $Q(U,V)$ is an arbitrary distribution over $U$ and $V$, and $KL(q||p)=\int q(x)log\frac{q(x)}{p(x)}dx$ is the \textit{Kullback-Leibler(KL) divergence} from $q$ to $p$, a measure of information lost when $p$ is used to approximate $q$. Although this interpretation is not so intuitive and requires understanding of entropy in information theory, it relates to a more intuitive measure-theoretic notion of distance between two measures, called \textit{total variational distance}, by \textit{Pinsker's Inequality}:

\begin{def*}[Total Variational Distance] 
For $P,Q$ two probability measures on measure sapce $(\chi,\mathcal{A})$ we define the \textit{total variational distance} $\|P-Q\|_{TV}=\sup\limits_{A\in\mathcal{A}}|P(A)-Q(A)|$
\end{def*}

\begin{lemma*}[Pinsker's Inequality]
Suppose $P,Q$ have a common dominating measure $\mu$ in $(\chi,\mathcal{A})$. Then
\begin{center}
$\|P-Q\|_{TV} \leq \sqrt{\frac{1}{2}KL(P||Q)}$
\end{center}
\end{lemma*}

Hence we see that maximising $\mathcal{F}(Q(U,V))$ with respect to $Q(U,V)$, and hence minimising the KL divergence from $Q(U,V)$ to $P(U,V|R)$, is sensible when trying to approximate $P(U,V|R)$ with $Q(U,V)$.Now $KL(P||Q)} \geq 0$ for all distributions $P,Q$, with equality if and only if $P \equiv Q$, known as \textit{Gibb's Inequality} in information theory. Hence the minimiser of the KL divergence from $Q(U,V)$ to $P(U,V|R)$ is $P(U,V|R)$ itself. However we have from above that $P(U,V|R)$ is intractable, hence this global minimiser of the KL divergence is not what we desire. We want instead something similar to a "local minimum" but in the infinite dimensional space of all distributions. A common practice in Bayesian variational inference when the posterior is intractable, is to constrain the approximator $Q$ to be of a particular form, and minimise the KL divergence with respect to $Q$ subject to this constraint, so that equality is not achieved above. The paper applies the \textit{mean field} approximation where $Q(U,V)$ is assumed to take the form $Q(U)Q(V)$. In other words, the posteriors of U and V are assumed to be independent when searching for the approximation. The reasoning behind using this constraint, apart from being simple and perhaps most natural, is that it makes the minimiser of KL divergence computationally tractable, so that we can find a closed form iterative scheme to approximate the minimising $Q(U,V)$: we may simply maximise with respect to $Q(U)$ and $Q(V)$ separately. optimising one keeping the other fixed, and iterate until convergence.

Equations (16,17,18,19,20,21) clarifying the 'u_i bar's 'v_i bar's to be approximated by the observed U_i V_i, clarify Phi and Psi's, and add in/take away transposes to get the correct dimensions for each term.

However, this assumption that the posterior of $U$ and $V$ are independent is difficult to justify since $U_i$ and $V_j$ depend heavily on each other through the observed rating $R_{ij}$. Hence due to this crude approximation we may face performance losses on the test set, which is the main drawback of this variational algorithm.

The VB algorithm does not only maximise $\mathcal{F}(Q(U)Q(V))$ with respect to $Q(U),Q(V)$, but also tries to learn the variance parameters $\Theta=\{\tau,\mathbf{\sigma},\mathbf{\rho} \}$. This is done by alternating the above maximisation of $\mathcal{F}(Q(U)Q(V))$ with respect to $Q(U)$ and $Q(V)$ with the maximisation (of the same objective) with respect to $\Theta$. By setting each of the derivatives to 0, we get the maximisers:

Equations (22,23,24)

Here we note that the VB algorithm is a \textit{Generalised Expectation-Maximisation}\ algorithm applied to the hidden variables $Q(U),Q(V)$ and the set of parameters $\Theta$. For the E-step, we maximise $\mathcal{F}(Q(U)Q(V))$ with respect to $Q(U),Q(V)$ by minimising $KL(Q(U)Q(V)||P(U,V|R))$, and for the M-step we maximise $\mathcal{F}(Q(U)Q(V))$ with respect to $\Theta$. For each step $\mathcal{F}(Q(U)Q(V))$ always increases, with log-likelihood staying the same for the E-step, whereas the KL divergence and log-likelihood may either increase or decrease for the M-step, since both are parametrised by $\Theta$. Figure [ ] displays the algorithm visually.

Figure 2.2 from variational algorithms for approximate bayesian inference.

The maximisation of $\mathcal{F}(Q(U)Q(V))$ with respect to $Q(U),Q(V)$ and $\Theta$ can be seen as a type of maximum-likelihood approach with penalisation by how much $Q(U)Q(V)$ and $P(U,V|R)$ differ, their KL divergence. By maximising $\mathcal{F}(Q(U,V))$ with respect to both sets of arguments, we are trying to maximise the likelihood such that $KL(Q(U)Q(V)||P(U,V|R))$ does not get too large, which is reasonable since we wish to learn $\Theta$ so that it fits the data well but at the same time wish to learn $Q(U)Q(V)$ to be a good approximator to $P(U,V|R)$.

In summary, the variational algorithm runs as follows:

E-step: (section 4.1 of paper)
M-step: (section 4.2 of paper)

The variables that need to be stored for each E-M step are $\Phi_i,\overbar{U_i},\Psi_j,\overbar{V_j} \forall i\in \{1,...,N\},j\in \{1,...,M\}$. Note however that the number of users $N>400,000$ and each $\Phi_i$ is of size $D \times D$, so we will not have enough memory to store all the $\Phi_i$'s. To alleviate this, we instead use $\Phi_i$ wherever it is required as soon as it is computed, then we discard it immediately for each $i\in \{1,...,N\}$. Note however that $\Psi_j \forall j\in \{1,...,M\} $ can be stored, since $M<20,000$. As we shall see in the pseudocode below, all that is needed to initialise VB is $U,V,\Psi,\mathbf{\sigma}$ and $\tau$. Also, instead of using $S_j$ and $t_j$ we update as we go $\Psi$ and $V'$, a container for the updated $V$.If initial values of $U$ and $V$ are unspecified, we initialise them such that each entry is sampled independently from the standard Gaussian.

Also note that we have $D$ spare degrees of freedom when choosing $U$ and $V$, since multiplying a column of $U$ by $c$ and dividing the corresponding column of $V$ by $c$, the product $UV^T$ is identical. Thus we may keep $\rho_l^2=\frac{1}{D}$ fixed, and initialise $U$ and $V$ by normalising each column of $V$ so that it has $L^2$ norm $\frac{1}{\sqrt{D}}$ and multiplying each column of U by an appropriate factor to keep $UV^T$ the same. Then since $diag(\sigma_1,...,\sigma_D)$ is meant to the covariance of each $U_i$ we initialise $\mathbf{\sigma}$ to be the diagonal terms of the sample covariance of the $U_i$'s. $\tau^2$ is initialised to 0.456 through experimentation according to the paper.

-Algorithm pseudocode-
Inputs: training_data,test_data,maxepoch,D (U,V,Psi,sigma,tau optional)

Initialise $U_i,V_j,\Psi_j,\tau,\mathbf{\sigma} \forall i\in \{1,...,N\},j\in \{1,...,M\}$ with suitable normalisations applied to columns of $U$ and $V$.

for t=1:maxepoch
	for j=1:M
		Initialise $V'_j$ to a vector of $D$ zeros (container for new V_j)
		Set $\Gamma_j=\Psi_j+V_jV_j^T$
		Reinitialise $\Psi_j=DI_D$
	end
	
	Initialise $\mathbf{\sigma}_{new}$ to a vector of $D$ zeros
	Intialise $\tau_{new}$ to 0.
	for i=1:M
		Set $\Phi=\left \( diag(\mathbf{\sigma})+\sum_{j \in N(i)}  \frac{\Gamma_j}{\tau^2} \right \)^{-1}$
		Set $U'=\left \( \sum_{j \in N(i)} \frac{R_{ij}V_j}{\tau^2} \right \)\Phi^T$ (container for new U_i)
		Set $\Omega=\Phi+U'U^T$
		Update $\sigma_{new}$=$\mathbf{\sigma}_{new}+U'^T.^2+diag(Phi)$ (where	.^2 squares each element of a vector)
		for $j \in N(i)$
			Update $\Psi_j=\Psi_j+\frac{\Omega}{\tau}$
			Update $V'_j=V'_j+\frac{R_{ij}U'}{\tau}$
			Update $\tau_{new}=\tau_{new}+tr(\Omega\Gamma_j)+R_{ij}^2-2R_{ij}V_j U'^T$
		end
		Update $U_i=U'$
	end
	for j=1:M
		Update $Psi_j=(\Psi_j)^{-1}$
		Update $V_j=V'_j \Psi_j$
	end
	Update $\mathbf{sigma}=\frac{\mathbf{\sigma}_{new}}{N-1}$
	Update $\tau=\frac{\tau_{new}}{K-1}$ where $K$ is the total number of observations in training_data
end
----------------------

Note that the time complexity for each epoch of the algorithm is $O(K+ND^3+MD^3)$, where the $D^3$ comes from the inversion of a $D \times D$ matrix. Having to compute this for each user is the bottleneck for the algorithm.

5.EXPERIMENTAL RESULTS

Information about actual competition with original test set. Distinguish from probe set.

BayesPMF as function of n_samples: 4,8,16,32 for each of 30D and 60D

Infrequent users: plot using a graph - x-axis: number of observed ratings (1-5,6-10, -20, -40, -80,-160, -320 -640 >641) for 60D values as in figure 4 right panel of BayesPMF

FOR RESULTS on PMF:
-mention overfitting for pmf. metion that higher dimensions give worse predictions.
-comment on use of sigmoid function and explain why it does not give better results.
---sigma must be small to guarantee that the ratings fall in [0,1] with high probability. However, with small sigma, the problem is that the extreme ratings 1,5 become very difficult to predict, as this will require a very large UiVj' with a small variance.
-talk about how learning U,V st it predicts R-mean(R) gives better results, and suggest an explanation.

For RESULTS on BayesPMF:
-mention that unlike PMF, BayesPMF does not seem to overfit as D increases, and instead performance seems to steadily grow as complexity increases. Hence increasing D to be greater than 60 is likely to yield higher performance, although Gibbs sampling does become much more computationally expensive. Paper mentions that for D=150 and 300, we get RMSE's close to 0.89. 
-mention results could be improved if one includes a burn-in period. Paper uses burn in of 800. Of course then the algorithm will be computationally very expensive.

For RESULTS on VB:
-VB random init does much better than stated in the paper. For 30D in paper: around 0.9190. In experiment: 0.9132.
-VB tends to perform better for higher D (higher model complexity, but not as much as for BayesPMF), but does overfit very quickly for D=60.
-mention VB applied to PMF or BayesPMF initialised values of U,V not successful. Relate to how entries of U and V for PMF/BayesPMF are small, unlike for VB. Maybe this is why. Also with VB the RMSE does not start off from the RMSE of the initial U,V. This might be another explanation.
-also VB does much worse when we try to have U,V predict R_ij-mean(R). For random init 30D, the RMSE goes down to 0.9878 at epoch 9 and stays near it.


So VB and PMF/BayesPMF have different preferences. But it seems as though VB initialised BayesPMF does okay. Perhaps we could do better if somehow we can transfer U,V for predicting R to U',V' predicting R-mean(R)? But this is not possible in general.

30D: Netflix baseline score - Cinematch,PMF, rawPMF, BayesPMF, rawBayesPMF, VB Random INIT, rawBayesPMF_VBinit
60D: Same
(Display as two graphs, up to 30epochs on x-axis)

6.FURTHER APPROACHES
-VB inference for the Hierarcial Bayesian model in BayesPMF.
-

Seems as though the variance of R_{ij} will be important for creating recommender systems, as people have limited time and cannot watch many movies. So want to recommend movies with not only a high expected rating but also a small variance, so that the user will be satisfied with high prob. 
We could measure this variance by Var_{U,V|R}(U_iV_j^T) or by Var(R|U,V). Note that for the above approaches, p(U,V|R) is intractable, and so is Var_{U,V|R}(U_iV_j^T). However for VB, we estimate the posterior by Q(U)Q(V), for which we know the variances (Phi and Psi), hence we know the variance of U_iV_j^T under the independence assumptions. So one could say that VB inference provides us with more information concerning posterior variances than with PMF or BayesPMF.
However it seems more intuitive to use Var(R|U,V) as the measure of variance we are interested in. Hence we may want to model the conditional distribution of R given U,V as:
R_{ij}|U,V ~ N(U_iV_j^T,f(sigma_i,rho_j))
where sigma_i and rho_j are some parameters inherent to each user/movie, which can be interpreted as a measure of "rationality" of a given user, or how much a movie makes the viewer irrational. With this interpretation, the current model for the distribution of R|U,V assumes that all users give ratings "rationally" or at least with the same degree of rationality, since Var(R_{ij}|U,V) is the same for all (ij). Hence were we to learn parameters based on data where some users have given ratings completely at random, not only will this model not be able to tell, but it will also provide predictions for rational users which have been harmed by these random ratings. 
So we can say that By introducing a Bayesian model with different variance parameters for each user/movie, we may be able to build a more intelligent system which can automatically learn these parameters and distinguish users for which successful recommendation will be very difficult.

Of course then there remains the issue of how to give weights to mean and variance for recommendation, ideally one which makes learning easier. for which there is a wide scope of research.


